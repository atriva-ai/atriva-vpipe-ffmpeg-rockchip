services:
  frontend:
    build:
      context: ./mobile-lp-recorder-ui
    ports:
      - "3000:3000"
    environment:
      - INTERNAL_API_URL=http://nginx
      - NEXT_PUBLIC_API_URL=http://localhost
    volumes:
      - frontend_node_modules:/app/node_modules
    depends_on:
      - backend
    networks:
      - lpr-net

  backend:
    build:
      context: ./dashboard-backend
    env_file:
      - ./dashboard-backend/.env
    environment:
      - VIDEO_PIPELINE_URL=http://video_pipeline:8002
    ports:
      - "8000:8000"
    depends_on:
      - db
    networks:
      - lpr-net

  db:
    image: postgres:15.5
    ports:
      - "5432:5432"
    env_file:
      - ./dashboard-backend/.env
    volumes:
      - pgdata:/var/lib/postgresql/data
    networks:
      - lpr-net

  ai_inference:
    build: ./services/ai-inference
    ports:
      - "8001:8001"
    env_file:
      - ./services/ai-inference/.env
    volumes:
      - shared-temp-data:/app/shared
    extra_hosts:
      - "localhost:host-gateway"
    networks:
      - lpr-net

  video_pipeline:
    build: ./services/video-pipeline
    ports:
      - "8002:8002"
    env_file:
      - ./services/video-pipeline/.env
    volumes:
      - shared-temp-data:/app/shared
    extra_hosts:
      - "localhost:host-gateway"
    networks:
      - lpr-net

  nginx:
    image: nginx:1.25.3-alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/conf.d:/etc/nginx/conf.d:ro
    depends_on:
      - backend
      - video_pipeline
      - ai_inference
    networks:
      - lpr-net

  #analytics_engine:
  #  build: ./services/analytics-engine
  #  ports:
  #    - "8003:8003"
  #  networks:
  #    - retail-net

volumes:
  pgdata:
  shared-temp-data:
  frontend_node_modules:

networks:
  lpr-net:
    driver: bridge